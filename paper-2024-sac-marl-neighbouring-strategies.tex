\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{mytodonotes}

% Copyright
%\setcopyright{none}
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{xx.xxx/xxx_x}

% ISBN
\acmISBN{979-8-4007-0629-5/25/03}

%Conference
\acmConference[SAC'25]{ACM SAC Conference}{March 31 –April 4, 2025}{Sicily, Italy}
\acmYear{2025}
\copyrightyear{2025}


\acmArticle{4}
\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
%\editor{Jennifer B. Sartor}
%\editor{Theo D'Hondt}
%\editor{Wolfgang De Meuter}


\begin{document}
\title{Neighbor-Based Decentralized Training Strategies for Multi-Agent Reinforcement Learning}
\titlenote{Produces the permission block, and
  copyright information}
\subtitle{Full Paper}
\subtitlenote{The full version of the author's guide is available as
  \texttt{acmart.pdf} document}
  
\renewcommand{\shorttitle}{SIG Proceedings Paper in LaTeX Format}


% \author{Nicolò Malucelli}
% \orcid{1234-5678-9012}
% \affiliation{%
%   \institution{University of Bologna}
%   \streetaddress{Via dell'Università, 50}
%   \city{Cesena} 
%   \country{Italy}
% }
% \email{nicolo.malucelli@studio.unibo.it}

% \author{Davide Domini}
% \orcid{1234-5678-9012}
% \affiliation{%
%   \institution{University of Bologna}
%   \streetaddress{Via dell'Università, 50}
%   \city{Cesena} 
%   \country{Italy}
% }
% \email{davide.domini@unibo.it}

% \author{Gianluca Aguzzi}
% \affiliation{%
%   \institution{University of Bologna}
%   \streetaddress{Via dell'Università, 50}
%   \city{Cesena} 
%   \country{Italy}
% }
% \email{gianluca.aguzzi@unibo.it}

% \author{Mirko Viroli}
% \affiliation{%
%   \institution{University of Bologna}
%   \streetaddress{Via dell'Università, 50}
%   \city{Cesena} 
%   \country{Italy}
% }
% \email{mirko.viroli@unibo.it}

% % The default list of authors is too long for headers}
% \renewcommand{\shortauthors}{N. Malucelli et al.}

\author{Anonymous Author}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Anonymous university}
  \country{Anonymous location}
}
% \email{anonymous.author@studio.unibo.it}
\renewcommand{\shortauthors}{A. Author et al.}

\begin{abstract}
Multi-agent deep reinforcement learning (MADRL) is gaining popularity as a powerful extension of
  single-agent reinforcement learning for training collaborative or competitive agents
  in complex environments. 
%
MADRL, however, introduces unique challenges, such as non-stationarity, partial observability, 
  and credit assignment that complicate the learning process.
%
Typically, in the literature, centralized training methods are used to tackle some of these challenges, 
  although they often face scalability limitations as the number of agents grows.
%
For this reason, decentralized training techniques have been proposed, however, 
  generally they have a longer convergence time compared to centralized ones 
  and may lead to instability.
%
This paper investigates the effectiveness of various neighbor-based decentralized training strategies based on the well-known deep-q learning algorithm 
  as a viable alternative to centralized training. 
%  
We evaluate experience sharing, nearest neighboring averaging, and nearest neighboring consensus methods 
  in a custom multi-agent environment and compare their performance against centralized training. 
%  
Our results show that neighbor-based methods can achieve comparable performance to centralized training 
  while offering improved scalability and communication efficiency. 
%
% Finally, we discuss the trade-offs between these methods and provide insights into their applicability 
%   in different scenarios.
\end{abstract}

%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_sigagentnificance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>  
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}


\keywords{Multi-Agent Reinforcement Learning, Decentralized Training, Neighbor-Based Methods, Scalability, Communication Efficiency}
\maketitle

\section{Introduction}

Distributed artificial intelligence has emerged as a prominent research area in recent years, 
evolving from its roots in multi-agent systems engineering 
through advancements in distributed learning. 
%
This evolution has encompassed collective behavior learning (e.g., COIN) 
and culminated in the modern field of multi-agent deep reinforcement learning (MADRL). 
%
MADRL has gained significant traction due to its flexibility and capacity to learn diverse behaviors, 
ranging from cooperative to competitive, 
as demonstrated in various case studies involving video games, swarm robotics, 
and traffic management. 
%
This paper focuses on the subdomain of cooperative multi-agent reinforcement learning, 
a particularly active and impactful area of research within MADRL.

Many prominent approaches in this field employ a centralized training and decentralized execution paradigm
 (e.g., COMA, MAPPO, MADPG), 
 where a centralized learner trains distributed policies that are subsequently executed independently. 
 While facilitating decentralized runtime operation and avoiding single points of failure, 
 this approach typically entails an offline learning phase (using simulators and centralized computation) 
 followed by online execution with no further learning. 
 This inherent limitation hinders adaptation to changing environments or tasks at runtime.  
 While fully independent learners offer a potential solution for online learning,
 they often underperform centralized counterparts due to instability in training and a lack of differentiation between agents and their interactions with the environment. 
 %
 However, in many real-world multi-agent systems, 
 agents only interact with a limited subset of the overall system, 
 as exemplified by limited sensing ranges in swarm robotics or localized interactions.  
 This observation also holds true in other natural systems, such as animal herds.


Therefore, 
this paper proposes and formalizes a family of distributed learning methodologies based on neighborhood information---in this research area is typically referred to as \emph{networked agents}.
This approach aims to: 
\begin{itemize}
  \item incorporate richer information compared to independent learners, and
  \item facilitate the emergence of collective behaviors through local interactions.
\end{itemize}
Specifically, we investigate three distinct neighboring-based training strategy encompassing both neural network-based approaches 
(e.g., neighboring averaging and consensus) 
and experience-based methods inspired by centralized experience sharing. 
We demonstrate the effectiveness of these strategies in comparison to both centralized and fully distributed approaches, 
highlighting how neighborhood-based policies offer a compelling compromise between these two extremes.

The remainder of this paper is structured as follows.
Section~\ref{sec:background} provides an overview of multi-agent reinforcement learning,
highlighting the key challenges and training paradigms.
Section~\ref{sec:neighboring} introduces the proposed neighboring-based training strategies,
detailing their implementation and operation.
Section~\ref{sec:experiments} presents the experimental setup and evaluation metrics.
Section~\ref{sec:results} discusses the results of the experiments,
comparing the performance of the neighboring-based methods against centralized training.
Finally, Section~\ref{sec:conclusion} concludes the paper,
summarizing the key findings and outlining potential future research directions.

\section{Background and Motivation}\label{sec:background}
Multi-agent reinforcement learning is an extension of a sequientail decision-making process, 
where multiple agents interact with each other and the environment.
In particular, each agent in the system aims to maximize its own reward,
which is typically a function of the global state (sometime not known, aka partial observable) and the joint action taken by all agents.
%
In doing that, each agent perceive the environment through its own observation function,
which maps the global state to the agent's local observation and perform an action based on its policy.
%
This action is then combined with the actions of other agents to form a joint action,
which is used to update the global state and reward.

MADRL is a subfield of reinforcement learning that focuses on training multiple agents to interact with each other within a shared environment.
%
In this \emph{deep} variant, agents are typically represented by neural networks that learn to maximize a shared (or individual) reward signal.
% 
%
MADRL is a wide area, and it encompassed a variety of scenarios, 
such as cooperative (e.g., multi-agent pathfinding), 
competitive (e.g., multi-agent games), 
and mixed environments.
%
In this paper, we focus on cooperative scenarios, where agents must collaborate to achieve a common goal,
introducing a common formalization (networked agents) and discussing the key concepts of joint action space, shared reward, partial observability, and credit assignment, following by different training schemes (centralized and distributed), with a discussion on the state of the art and motivation for our work.
\subsection{Formalization}
Since MARL is a broad field, it is essential to provide a formalization to guide the discussion.
In this paper, we consider a partially observable networked multi-agent system (PONMAS) as a tuple $(\mathcal{G}, \mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{P}, \mathcal{R}, \gamma)$, where:
$\mathcal{G} = (N, E)$ is a communication graph, where $N = {1, \dots, n}$ is the set of $n$ agents and $E \subseteq N \times N$ represents the communication links between agents. Time-varying graphs $\mathcal{G}_t = (N, E_t)$ can be used to represent communication evolving over time $t$.
%
$\mathcal{S}$ is the global state space.
%
$\mathcal{A} = \mathcal{A}^1 \times \dots \times \mathcal{A}^n$ is the joint action space, where $\mathcal{A}^i$ is the action space of agent $i$.
%
$\mathcal{O} = \mathcal{O}^1 \times \dots \times \mathcal{O}^n$ is the joint observation space, where $\mathcal{O}^i$ is the observation space for agent $i$. 
%
Observations $o^i \in \mathcal{O}^i$ provide partial information about the global state.
$\mathcal{P}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to [0, 1]$ is the state transition function, describing the probability of transitioning to a new state $s' \in \mathcal{S}$ given the current state $s \in \mathcal{S}$ and joint action $a \in \mathcal{A}$.
%
$\mathcal{R} = {\mathcal{R}^i}_{i \in N}$, where $\mathcal{R}^i: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ is the reward function for agent $i$.
%
$\gamma \in [0, 1]$ is the discount factor.
%
Each agent $i$ at time $t$ receives an observation $o^i_t \in \mathcal{O}^i$, takes an action $a^i_t \in \mathcal{A}^i$ based on its local policy $\pi^i: \mathcal{O}^i \times \mathcal{A}^i \to [0,1]$, and receives a reward $r^i_{t+1} = \mathcal{R}^i(s_t, a_t)$. 
%
The global state evolves according to $\mathcal{P}$. 
Agents can exchange information with their neighbors in $\mathcal{G}$ (or $\mathcal{G}t$ if the graph is time-varying). 
%
The objective is typically to maximize the expected discounted sum of rewards $\sum{t=0}^{\infty} \gamma^t r^i_t$ for each agent $i$, 
or some global reward function like the average reward over all agents.
Formally, it can be defined as:
\[ J(\{\pi^i\}) = \lim_{T \to \infty} \mathbb{E} \left[ \frac{1}{T} \sum_{t=0}^{T-1} \frac{1}{N} \sum_{i \in \mathcal{N}} R^i(s_t, a_t) \right] \]

In doing this, agents typically use other function to estimate the value of taking an action in a given state, such as the action-value function or Q-value. 
The latter represents the expected cumulative discounted reward the agent will receive by taking action $a \in \mathcal{A}$ in state $s \in \mathcal{S}$ and then following its policy $\pi^i$:

\[ Q^i(s, a) = \mathbb{E}_{\pi^i} \left[ \sum_{t=0}^{\infty} \gamma^t r^i_{t+1} | s_0 = s, a_0 = a \right] \]

Similar to the individual reward function, a global average reward function can be defined as $\bar{\mathcal{R}}(s, a) = \frac{1}{n} \sum_{i=1}^n \mathcal{R}^i(s,a)$, leading to a global average Q-value:

\[ \bar{Q}(s, a) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t \bar{\mathcal{R}}(s_t, a_t) | s_0 = s, a_0 = a \right] \]

where $\pi = \{\pi^i\}_{i \in N}$ represents the joint policy.  The goal in collaborative MARL is often to find a joint policy that maximizes this global average Q-value or a similar global objective.
%\subsection{Key Concepts}

\subsection{Learning Schemes}
Multi-agent learning can be categorized based on the training scheme and the information accessibility for agents. 
Two prominent paradigms are centralized training with decentralized execution (CTDE) and decentralized training with decentralized execution (DTDE).

\paragraph{Centralized Training with Decentralized Execution (CTDE)}
In CTDE, a central learner trains all agents, 
but execution of the learned policy is decentralized. 
This process typically involves two phases:
\begin{itemize}
    \item \textbf{Offline training:} agents interact with a simulator or environment, 
    gathering experience data used to update the central learner's policy.
    \item \textbf{Online execution:} agents deploy the learned policy in the environment without further learning.
\end{itemize}
This approach offers several benefits.  
Leveraging a centralized dataset allows for efficient learning, 
potentially exploiting global information, 
which is crucial in cooperative scenarios requiring coordinated actions. 
Furthermore, decentralized execution allows agents to act independently after training.
%
However, CTDE has limitations. 
Scalability can become problematic with a large number of agents. 
The reliance on a central learner hinders adaptation to dynamic environments or tasks during runtime.
\sloppy
\paragraph{Decentralized Training with Decentralized Execution (DTDE)}
DTDE eliminates the central learner; each agent learns independently. 
Inter-agent cooperation and communication can be incorporated during both training and execution. 
Unlike CTDE, training and execution phases are not necessarily separate. 
Simultaneous learning and execution allow for adaptation to changing environments or tasks at runtime.
%
Key advantages of DTDE include scalability with the number of agents and adaptability to dynamic environments.
%
However, DTDE faces challenges. 
Convergence time is often longer compared to centralized training. 
The lack of central coordination can lead to instability and difficulties in differentiating agents and their environmental interactions. 
Online learning in DTDE can introduce non-stationarity if agents do not account for other agents' evolving policies during training, 
as the environment effectively changes from each agent's perspective.
\subsection{State of the Art}
Multi-agent reinforcement learning is a longstanding research area with a rich literature, starting 
from seminal works like [cite ] and [cite] to more recent contributions like [cite] and [cite].
%
In the following, we briefly discuss some of the most relevant approaches in the field.
\paragraph{Independent Learners}

\paragraph{Centralized Actor-Critic Methods}

\subsection{Motivation}

\section{Neighboring-Based Distributed Learning Strategies}\label{sec:neighboring}
\begin{itemize}
\item Provide a detailed explanation of each neighbor-based method:
\begin{itemize}
\item \textbf{Independent learners:} Emphasize its simplicity and scalability but highlight its vulnerability to non-stationarity.
\item \textbf{NN-Averaging:} Describe the weight averaging process, discuss the influence of neighbor selection and averaging frequency, and mention weighted averaging variations.
\item \textbf{NN-Consensus:} Explain the selection of the best-performing network and its potential benefits and drawbacks.
\item \textbf{Experience Sharing:} Detail how agents share experiences, discuss asynchronous vs. synchronous sharing, and highlight its advantages in sparse interaction environments.
\end{itemize}
\end{itemize}
\section{Experimental Setup}\label{sec:experiments}
\begin{itemize}
\item Introduce the custom "collect the items" multi-agent environment:
\begin{itemize}
\item Describe the environment's dynamics, agent goals, and challenges.
\item Specify the action space (continuous and discrete) and observation space design.
\item Explain the reward structure, emphasizing the components that encourage cooperation.
\end{itemize}
\item Detail the implementation framework (Gymnasium, RLlib, PyTorch).
\item Specify the chosen RL algorithm (DQN) and its hyperparameter configuration.
\item Define the evaluation metrics: average episode length and average reward.
\end{itemize}

\section{Results and Evaluation}\label{sec:results}
\begin{itemize}
\item \textbf{Performance comparison:}
\begin{itemize}
\item Present the results of each distributed strategy against the centralized training baseline.
\item Use graphs and statistical analysis to compare the average episode length and average reward.
\item Highlight which neighbor-based methods perform comparably to centralized training.
\end{itemize}
\item \textbf{Scalability analysis:}
\begin{itemize}
\item Describe how the experiments were modified to assess scalability (increasing agents, spawn area).
\item Show how each method scales with the increasing complexity, using appropriate visualizations.
\item Discuss the implications of using pre-trained models with a larger number of agents.
\end{itemize}
\item \textbf{Communication overhead analysis:}
\begin{itemize}
\item Quantify the amount of information exchange for each strategy.
\item Compare the communication overhead across different neighborhood sizes, possibly using graphs.
\item Discuss the trade-offs between performance and communication efficiency.
\item Mention potential techniques for reducing communication overhead.
\end{itemize}
\end{itemize}

\section{Conclusion}\label{sec:conclusion}
\begin{itemize}
\item Summarize the key findings of the paper.
\item Emphasize the viability of neighbor-based methods as alternatives to centralized training in MARL.
\item Discuss the trade-offs observed in terms of performance, scalability, and communication overhead.
\item Suggest potential directions for future research, such as exploring hybrid approaches or more advanced communication protocols.
\end{itemize}


\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography} 

\end{document}
