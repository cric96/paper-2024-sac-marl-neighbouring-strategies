\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{mytodonotes}

% Copyright
%\setcopyright{none}
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{xx.xxx/xxx_x}

% ISBN
\acmISBN{979-8-4007-0629-5/25/03}

%Conference
\acmConference[SAC'25]{ACM SAC Conference}{March 31 –April 4, 2025}{Sicily, Italy}
\acmYear{2025}
\copyrightyear{2025}


\acmArticle{4}
\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
%\editor{Jennifer B. Sartor}
%\editor{Theo D'Hondt}
%\editor{Wolfgang De Meuter}


\begin{document}
\title{Neighbor-Based Decentralized Training Strategies for Multi-Agent Reinforcement Learning}
\titlenote{Produces the permission block, and
  copyright information}
\subtitle{Full Paper}
\subtitlenote{The full version of the author's guide is available as
  \texttt{acmart.pdf} document}
  
\renewcommand{\shorttitle}{SIG Proceedings Paper in LaTeX Format}


% \author{Nicolò Malucelli}
% \orcid{1234-5678-9012}
% \affiliation{%
%   \institution{University of Bologna}
%   \streetaddress{Via dell'Università, 50}
%   \city{Cesena} 
%   \country{Italy}
% }
% \email{nicolo.malucelli@studio.unibo.it}

% \author{Davide Domini}
% \orcid{1234-5678-9012}
% \affiliation{%
%   \institution{University of Bologna}
%   \streetaddress{Via dell'Università, 50}
%   \city{Cesena} 
%   \country{Italy}
% }
% \email{davide.domini@unibo.it}

% \author{Gianluca Aguzzi}
% \affiliation{%
%   \institution{University of Bologna}
%   \streetaddress{Via dell'Università, 50}
%   \city{Cesena} 
%   \country{Italy}
% }
% \email{gianluca.aguzzi@unibo.it}

% \author{Mirko Viroli}
% \affiliation{%
%   \institution{University of Bologna}
%   \streetaddress{Via dell'Università, 50}
%   \city{Cesena} 
%   \country{Italy}
% }
% \email{mirko.viroli@unibo.it}

% % The default list of authors is too long for headers}
% \renewcommand{\shortauthors}{N. Malucelli et al.}

\author{Anonymous Author}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Anonymous university}
  \country{Anonymous location}
}
% \email{anonymous.author@studio.unibo.it}
\renewcommand{\shortauthors}{A. Author et al.}

\begin{abstract}
Multi-agent reinforcement learning  is gaining popularity as a powerful extension of
  single-agent reinforcement learning for training collaborative or competitive agents
  in complex environments. 
%
MARL introduces unique challenges, such as non-stationarity, partial observability, 
  and credit assignment that complicate the learning process.  
%
Typically, in the literature, centralized training methods are used to tackle some of these challenges, 
  although they often face scalability limitations as the number of agents grows.
%
For this reason, decentralized training techniques have been proposed, however, 
  generally they have a longer convergence time compared to centralized ones 
  and may lead to instability.
%
This paper investigates the effectiveness of various neighbor-based decentralized training strategies 
  as a viable alternative to centralized training. 
%  
We evaluate Experience Sharing, Nearest Neighboring Averaging, and Nearest Neighboring Consensus methods 
  in a custom multi-agent environment and compare their performance against centralized training. 
%  
Our results show that neighbor-based methods can achieve comparable performance to centralized training 
  while offering improved scalability and communication efficiency. 
%
% Finally, we discuss the trade-offs between these methods and provide insights into their applicability 
%   in different scenarios.
\end{abstract}

%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>  
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}


\keywords{Multi-Agent Reinforcement Learning, Decentralized Training, Neighbor-Based Methods, Scalability, Communication Efficiency}
\maketitle

\section{Introduction}

Distributed artificial intelligence has emerged as a prominent research area in recent years, 
evolving from its roots in multi-agent systems engineering 
through advancements in distributed learning. 
%
This evolution has encompassed collective behavior learning (e.g., COIN) 
and culminated in the modern field of multi-agent deep reinforcement learning (MADRL). 
%
MADRL has gained significant traction due to its flexibility and capacity to learn diverse behaviors, 
ranging from cooperative to competitive, 
as demonstrated in various case studies involving video games, swarm robotics, 
and traffic management. 
%
This paper focuses on the subdomain of cooperative multi-agent reinforcement learning, 
a particularly active and impactful area of research within MADRL.

Many prominent approaches in this field employ a centralized training and decentralized execution paradigm
 (e.g., COMA, MAPPO, MADPG), 
 where a centralized learner trains distributed policies that are subsequently executed independently. 
 While facilitating decentralized runtime operation and avoiding single points of failure, 
 this approach typically entails an offline learning phase (using simulators and centralized computation) 
 followed by online execution with no further learning. 
 This inherent limitation hinders adaptation to changing environments or tasks at runtime.  
 While fully independent learners offer a potential solution for online learning,
 they often underperform centralized counterparts due to instability in training and a lack of differentiation between agents and their interactions with the environment. 
 %
 However, in many real-world multi-agent systems, 
 agents only interact with a limited subset of the overall system, 
 as exemplified by limited sensing ranges in swarm robotics or localized interactions.  
 This observation also holds true in other natural systems, such as animal herds.

Therefore, 
this paper proposes and formalizes a family of distributed learning methodologies based on neighborhood information. 
This approach aims to: 
\begin{itemize}
  \item incorporate richer information compared to independent learners, and
  \item facilitate the emergence of collective behaviors through local interactions.
\end{itemize}
Specifically, we investigate three distinct neighboring-based training strategy encompassing both neural network-based approaches 
(e.g., neighboring averaging and consensus) 
and experience-based methods inspired by centralized experience sharing. 
We demonstrate the effectiveness of these strategies in comparison to both centralized and fully distributed approaches, 
highlighting how neighborhood-based policies offer a compelling compromise between these two extremes.

The remainder of this paper is structured as follows.
Section~\ref{sec:background} provides an overview of multi-agent reinforcement learning,
highlighting the key challenges and training paradigms.
Section~\ref{sec:neighboring} introduces the proposed neighboring-based training strategies,
detailing their implementation and operation.
Section~\ref{sec:experiments} presents the experimental setup and evaluation metrics.
Section~\ref{sec:results} discusses the results of the experiments,
comparing the performance of the neighboring-based methods against centralized training.
Finally, Section~\ref{sec:conclusion} concludes the paper,
summarizing the key findings and outlining potential future research directions.

\section{Background}\label{sec:background}
\begin{itemize}
\item Introduce Multi-Agent Reinforcement Learning (MARL) and its specific challenges.
\item Provide a formalization (networked agents?) and explain the key concepts: joint action space, shared reward, partial observability, and credit assignment.
\item Explain different MARL training schemes: centralized training (CTCE, CTDE), distributed training (DTDE).
\end{itemize}

\section{Neighboring-Based Distributed Learning Strategies}\label{sec:neighboring}
\begin{itemize}
\item Provide a detailed explanation of each neighbor-based method:
\begin{itemize}
\item \textbf{Independent learners:} Emphasize its simplicity and scalability but highlight its vulnerability to non-stationarity.
\item \textbf{NN-Averaging:} Describe the weight averaging process, discuss the influence of neighbor selection and averaging frequency, and mention weighted averaging variations.
\item \textbf{NN-Consensus:} Explain the selection of the best-performing network and its potential benefits and drawbacks.
\item \textbf{Experience Sharing:} Detail how agents share experiences, discuss asynchronous vs. synchronous sharing, and highlight its advantages in sparse interaction environments.
\end{itemize}
\end{itemize}
\section{Experimental Setup}\label{sec:experiments}
\begin{itemize}
\item Introduce the custom "collect the items" multi-agent environment:
\begin{itemize}
\item Describe the environment's dynamics, agent goals, and challenges.
\item Specify the action space (continuous and discrete) and observation space design.
\item Explain the reward structure, emphasizing the components that encourage cooperation.
\end{itemize}
\item Detail the implementation framework (Gymnasium, RLlib, PyTorch).
\item Specify the chosen RL algorithm (DQN) and its hyperparameter configuration.
\item Define the evaluation metrics: average episode length and average reward.
\end{itemize}

\section{Results and Evaluation}\label{sec:results}
\begin{itemize}
\item \textbf{Performance comparison:}
\begin{itemize}
\item Present the results of each distributed strategy against the centralized training baseline.
\item Use graphs and statistical analysis to compare the average episode length and average reward.
\item Highlight which neighbor-based methods perform comparably to centralized training.
\end{itemize}
\item \textbf{Scalability analysis:}
\begin{itemize}
\item Describe how the experiments were modified to assess scalability (increasing agents, spawn area).
\item Show how each method scales with the increasing complexity, using appropriate visualizations.
\item Discuss the implications of using pre-trained models with a larger number of agents.
\end{itemize}
\item \textbf{Communication overhead analysis:}
\begin{itemize}
\item Quantify the amount of information exchange for each strategy.
\item Compare the communication overhead across different neighborhood sizes, possibly using graphs.
\item Discuss the trade-offs between performance and communication efficiency.
\item Mention potential techniques for reducing communication overhead.
\end{itemize}
\end{itemize}

\section{Conclusion}\label{sec:conclusion}
\begin{itemize}
\item Summarize the key findings of the paper.
\item Emphasize the viability of neighbor-based methods as alternatives to centralized training in MARL.
\item Discuss the trade-offs observed in terms of performance, scalability, and communication overhead.
\item Suggest potential directions for future research, such as exploring hybrid approaches or more advanced communication protocols.
\end{itemize}


\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography} 

\end{document}
